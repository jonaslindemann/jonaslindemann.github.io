{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Coding thoughts","text":"<p>My name is Jonas Lindemann and I work in HPC where I teach coding in C++, Python and Fortran. I also develop HPC-related software packages (GfxLauncher, ARC Storage Explorer and much more). In my research, I developed several Structural Mechanics applications, primarily for use in conceptual design and teaching. </p> <p></p> <p>This blog is an attempt to open up my development process and thoughts to a wider audience and also encourage input. Recently I have started the process of modernizing several of my older applications and also rewriting them with new concepts and ideas. </p> <p>Welcome!</p> <p>Jonas Lindemann</p>"},{"location":"gen-links/","title":"General","text":"<p>Modern Fortran Book - Online Ingenj\u00f6rens guide till Python Min LU Profil</p>"},{"location":"gh-links/","title":"Projects","text":"<p>ObjectiveFrame Modern Fortran Book - Source Qt Creator Fortran extensions ForcePAD 3 - Development Lightray - 2D rendering library based on Raylib ForcePAD 2.x Ivf++ - Interactive Visualisation Framework - 3D Scenegraph library Ivf2 - Modern reimplementation of Ivf++ OO Fortran examples and lecture GFX Launcher Toolkit CALFEM for Python</p>"},{"location":"yt-links/","title":"Videos","text":"<p>My YouTube channel</p>"},{"location":"yt-links/#courses","title":"Courses","text":""},{"location":"yt-links/#introduction-to-scientific-computing-2024","title":"Introduction to Scientific Computing - 2024","text":""},{"location":"yt-links/#scientific-programming-in-python-and-fortran-2023","title":"Scientific Programming in Python and Fortran - 2023","text":""},{"location":"yt-links/#interactive-software","title":"Interactive Software","text":""},{"location":"yt-links/#lunarc","title":"LUNARC","text":""},{"location":"yt-links/#fortran","title":"Fortran","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/","title":"The fun of coding 3D graphics in C++ and OpenGL","text":"<p>There exist several 3D graphics libraries for C++ using OpenGL such as Raylib, OpenSceneGraph, VTK, Magnum, Cinder, OpenFrameworks and many more.</p> <p>There also game engines using C++ such as Unreal Engine, Godot, CryEngine, Lumberyard and more.</p> <p>However, my needs for 3d programming have not been for games but for interactive 3d applications mainly in the field of engineering. I have used OpenGL for many years. For my PhD I developed a 3D Scene Graph library, Ivf++, which was a wrapper around OpenGL. It contained a set of nodes for implementing interactive 3D applications, such as ObjectiveFrame a 3D beam analysis application focusing on real-time interaction. </p> <p>During the last decade OpenGL has evolved and the fixed function pipeline has been deprecated. Modern OpenGL is based on shaders and the programmable pipeline. This has made OpenGL more powerful, but with the cost of complexity and ease of use. This article is about my journey implementing a new 3D graphics library for C++ that is easy to use and at the same time flexible. Ultimately I want to get back to ease of use of the fixed function pipeline but with the power of modern OpenGL.</p> <p>In an effort to learn modern OpenGL I started to reimplement the Ivf++ library using modern shader based OpenGL. I didn't want to refactor the original library as it is used in ObjectiveFrame, so I decided to create a new library from scratch, ivf2. At a high level I wanted to remove as many obstacles as possible programming 3D graphics applications in C++. The library should be easy to use and at the same time powerful. Also, it should be fun to create 3D applications in C++. Perhaps I will use it for creative coding at some point in the future.</p>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#design-goals","title":"Design goals","text":"<p>As the library is a work in progress, the design goals are not yet fully realized and will probarbly change over time. However, the following are some of the goals I have in mind:</p> <ul> <li>Easy to use - consistent syntax and API that is easy to understand)</li> <li>Modern OpenGL - support modern OpenGL features</li> <li>Scene Graph - main focus of rendering is the scene graph</li> <li>Rich set of primitives - support for common 3D primitives</li> <li>Interactive - support for easy interaction with the 3D scene</li> <li>Built-in window support (GLFW) - it should be easy to create a window in the library</li> <li>Build-in GUI support (ImGui) - immediate mode GUI for easy interaction with the application</li> <li>Fully object-oriented - use of classes and objects for easy programming</li> <li>Robust - shared and unique pointer support</li> <li>Built-in stock shaders - no need to implement shaders for simple applications</li> <li>Post processing effects - built-in support for post processing effects</li> </ul>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#the-journey-starts","title":"The journey starts","text":"<p>The first steps in implementing the library was to create a flexible infrastructure compiling and linking shaders and keeping track of these. Shaders support in ivf2 is implemented in number of classes:</p> <ul> <li>Shader - a class for loading and compiling a shader. </li> <li>Program - a class for linking shaders into a program. </li> <li>ShaderManager - a class for managing shaders and programs.</li> </ul> <p>The Shader class can load shaders from file and from strings. The built-in stock shaders are loaded from strings that are included from . The Program class links shaders into a program. The class also provides an interface to the shader uniforms. The following example shows how to create a simple shader program:</p> <pre><code>#include &lt;ivf/vertex_shader.h&gt;\n#include &lt;ivf/fragment_shader.h&gt;\n#include &lt;ivf/program.h&gt;\n#include &lt;ivf/stock_shaders.h&gt;\n\nint main() \n{\n    // Initialization\n\n    auto vertexShader = VertexShader::create(ivf::basic_vert_shader_source);\n    auto fragmentShader = FragmentShader::create(ivf::basic_frag_shader_source);\n\n    auto program = Program::create();\n\n    program.addShader(vertexShader);\n    program.addShader(fragmentShader);\n\n    if (!program.link())\n    {\n        std::cout &lt;&lt; \"Failed to compile and link program.\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n        // Main game loop\n\n    while (!WindowShouldClose()) \n    {\n        // Draw\n\n        program-&gt;use();\n        program-&gt;uniformFloat(\"time\", GetTime());\n        // Draw something\n    }\n\n    // De-Initialization\n\n    return 0;\n}\n</code></pre> <p>To make it even easier to load and manage shaders a special singleton class ShaderManager is used. The ShaderManager class is a singleton class that manages shaders and programs. The shader manager implements methods for directly loading and compiling shaders to named programs. The following example shows how to create a simple shader program using the ShaderManager:</p> <pre><code>#include &lt;ivf/shader_manager.h&gt;\n#include &lt;ivf/stock_shaders.h&gt;\n\nint main() \n{\n    // Initialization\n\n    auto shaderManager = ShaderManager::getInstance();\n    auto program = shaderManager-&gt;loadProgram(\"basic\", ivf::basic_vert_shader_source, ivf::basic_frag_shader_source);\n\n    if (shaderManager-&gt;compileLinkErrors())\n    {\n        std::cout &lt;&lt; \"Failed to compile and link program.\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    // Main game loop\n\n    while (!WindowShouldClose()) \n    {\n        // Draw\n\n        program-&gt;use();\n        program-&gt;uniformFloat(\"time\", GetTime());\n        // Draw something\n    }\n\n    // De-Initialization\n\n    return 0;\n}\n</code></pre> <p>Compiled programs can be convieniently accessed by name using the ShaderManager. </p> <pre><code>// Get named program\n\nauto program = shaderManager-&gt;program(\"basic\");\n\n// The the current program\n\nshaderManager-&gt;setCurrentProgram(\"basic\");\n\nauto program = shaderManager-&gt;currentProgram();\n</code></pre>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#not-calling-new-and-delete","title":"Not calling new and delete...","text":"<p>One major goal of the library was to be able to use shared and unique pointers for managing objects. This means that you don't have to call new and delete for creating objects. The library is fully object-oriented and uses shared and unique pointers for managing objects. </p> <p>To support shared pointers and also supporting new users I decided that all classes implemented by the library should have a static create method, which can be called to create an object. By combining this with the auto keyword in C++11, the code becomes very clean and easy to read. The following example shows how to create a shared pointer to a ivf::Cylinder object:</p> <pre><code>using namespace ivf;\n\nauto cylinder = Cylinder::create(1.0, 1.0, 32);\n</code></pre> <p>The create method is a static method that returns a shared pointer to the object. The object is automatically deleted when the last shared pointer goes out of scope.</p> <p>The syntax of using a create() method is heavily inspired by Object Pascal and Delphi. I have used Delphi for many years and I like the syntax of creating objects in Delphi.</p> <p>To make it easier for new users of the library, all classes also defined a typedef for a shared pointer to the object. This means that you can use the typedef to create a shared pointer to the object. The following example shows how to create a shared pointer to a ivf::Cylinder object using the typedef:</p> <pre><code>using namespace ivf;\n\nCylinderPtr cylinder = Cylinder::create(1.0, 1.0, 32);\n\n// Equivalent to\n// std::shared_ptr&lt;Cylinder&gt; cylinder = std::make_shared&lt;Cylinder&gt;(1.0, 1.0, 32);\n</code></pre> <p>As the library heavily relies on Polymorphism, using shared pointers is a natural choice. The library is designed to be used with shared pointers and the library itself uses shared pointers for managing objects.</p>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#the-scene-graph","title":"The Scene Graph","text":"<p>The scene graph in ivf2 is based on 2 main classes Node and CompositeNode. The Node class is the base class for all nodes in the scene graph. The CompositeNode class is a node that can contain other nodes. The scene graph is a tree structure where each node can have a parent and children. The scene graph is traversed in a depth-first manner. The following example shows how to create a simple scene graph:</p> <pre><code>#include &lt;ivf/gl.h&gt;\n#include &lt;ivf/nodes.h&gt;\n\nint main() \n{\n    // ...\n\n    auto scene = CompositeNode::create();\n    auto cube = Cube::create();\n    auto sphere = Sphere::create();\n\n    scene-&gt;add(cube);\n\n    // ...\n\n    scene-&gt;draw();\n\n    // ...\n\n    return 0;\n}\n</code></pre> <p>Most primitive classes derive from the TransformNode class which provides basic transformation functionality. The TransformNode class is a node that can be transformed using translation, rotation and scaling. The following example shows how to create a simple scene graph with a transformed cube:</p> <pre><code>auto scene = CompositeNode::create();\nauto cube = Cube::create();\nauto sphere = Sphere::create();\n\ncube-&gt;setPos(glm::vec3(0.0, 1.0, 0.0));\nsphere-&gt;setPos(glm::vec3(1.0, 0.0, 0.0));\n</code></pre>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#meshnode-and-primitives","title":"MeshNode and primitives","text":"<p>I realised the time for immediate mode rendering is over. The library is based on modern OpenGL and uses Vertex Buffer Objects (VBO) and Vertex Array Objects (VAO) for rendering. The MeshNode class implements the functionality for rendering creating meshes using VBOs and VAOs using a simple interface similar to the fixed function pipeline. To implement a mesh, a derived class overrides the doSetup method of the MeshNode-class. The following example shows how the Cube class implements its mesh:</p> <pre><code>void Cube::doSetup()\n{\n    //   y  o--------o\n    //   ^ /|       /| \n    //   |/ |     2/ |\n    //  3o--------o  |\n    //   |  o-----|--o \n    //   | /      | / \n    //   |/       |/\n    //   o--------o --&gt; x\n    //   0        1\n\n    double n = m_size/2.0;\n\n    mesh()-&gt;begin(GL_TRIANGLES);\n    mesh()-&gt;vertex3d(-n, -n, n);\n    mesh()-&gt;color3f(1.0f, 0.0f, 0.0f);\n    mesh()-&gt;vertex3d(n, -n, n);\n    mesh()-&gt;color3f(1.0f, 0.0f, 1.0f);\n    mesh()-&gt;vertex3d(n, n, n);\n    mesh()-&gt;color3f(1.0f, 1.0f, 0.0f);\n    mesh()-&gt;vertex3d(-n, n, n);\n    mesh()-&gt;color3f(1.0f, 1.0f, 1.0f);\n\n    mesh()-&gt;index3i(0, 1, 2); // front\n    mesh()-&gt;index3i(0, 2, 3);\n\n    mesh()-&gt;vertex3d(-n, -n, -n);\n    mesh()-&gt;color3f(0.0f, 0.0f, 1.0f);\n    mesh()-&gt;vertex3d(n, -n, -n);\n    mesh()-&gt;color3f(0.0f, 1.0f, 0.0f);\n    mesh()-&gt;vertex3d(n, n, -n);\n    mesh()-&gt;color3f(0.0f, 1.0f, 1.0f);\n    mesh()-&gt;vertex3d(-n, n, -n);\n    mesh()-&gt;color3f(1.0f, 0.0f, 0.0f);\n\n    mesh()-&gt;index3i(4, 6, 5); // back\n    mesh()-&gt;index3i(4, 7, 6);\n\n    // ...\n\n    mesh()-&gt;end();\n}\n</code></pre> <p>I also realised that implementing all primitives in the library would be a huge task. To solve this I search the internet and github for a solution to this problem. What I found was a geometry generator called Generator. It is a header only library that can generate most basic primitives shapes that can be used in 3D graphics. Most of the primitives in ivf2 are implemented using the Generator library. The following example shows how to create a sphere using the Generator library:</p> <pre><code>// ...\n\nvoid Sphere::doSetup()\n{\n    SphereMesh sphere(m_radius, m_slices, m_segments, m_sliceStart, m_sliceSweep, m_segmentStart, m_segmentSweep);\n\n    AnyGenerator&lt;MeshVertex&gt; vertices = sphere.vertices();\n    AnyGenerator&lt;Triangle&gt; triangles = sphere.triangles();\n\n    this-&gt;createFromGenerator(vertices, triangles);\n}\n\n// ...\n</code></pre> <p>To support generating geometry using this library the MeshNode class implements a special method createFromGenerator that can create a mesh from a generator. The createFromGenerator method takes two generators, one for vertices and one for triangles. </p>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#lights-materials-and-textures","title":"Lights, materials and textures","text":"<p>To recreate an environment similar to the fixed function pipeline, ivf2 comes with built-in support for lights, materials and textures. The Light class implements a light source that can be used in the scene. The Material class implements a material that can be used to set the material properties of an object. The Texture class implements a texture that can be used to texture an object. The following example shows how to create a light, material and texture:</p> <pre><code>auto shaderMgr = ShaderManager::create();\n\n// Load our basic shader\n\nshaderMgr-&gt;loadBasicShader();\n\n// Lights are managed by the LightManager\n\nauto lightMgr = LightManager::create();\n\n// Add a point light\n\nauto pointLight1 = lightMgr-&gt;addPointLight();\npointLight1-&gt;setEnabled(true);\npointLight1-&gt;setDiffuseColor(glm::vec3(1.0, 1.0, 1.0));\npointLight1-&gt;setSpecularColor(glm::vec3(1.0, 1.0, 1.0));\npointLight1-&gt;setAttenuation(1.0, 0.0, 0.0);\npointLight1-&gt;setPosition(glm::vec3(5.0, 5.0, 5.0));\n\n// Update the shader\n\nlightMgr-&gt;apply();\n\n// Create a material\n\nauto material = Material::create();\nmaterial-&gt;setDiffuseColor(glm::vec4(random(0.0, 1.0), random(0.0, 1.0), random(0.0, 1.0), 1.0));\nmaterial-&gt;setAmbientColor(glm::vec4(0.1, 0.1, 0.1, 1.0));\n\n// Create a sphere with an attached material\n\nauto sphere = Sphere::create();\nsphere-&gt;setMaterial(material);\n\n// ...\n</code></pre> <p>Textures can also be attached to nodes in the scene graph. The Texture class implements a texture that can be used to texture an object. The following example shows how to create a texture and attach it to a node:</p> <pre><code>auto textureCat = Texture::create();\ntextureCat-&gt;load(\"assets/pop_cat.png\");\n\nauto textureBrick = Texture::create();\ntextureBrick-&gt;load(\"assets/brick.png\");\n\nauto sphere = Sphere::create();\nsphere-&gt;setTexture(textureCat);\nsphere-&gt;setMaterial(sphereMaterial);\nsphere-&gt;setPos(glm::vec3(0.0, 3.0, 0.0));\n\nauto box = Box::create();\nbox-&gt;setTexture(textureBrick);\nbox-&gt;setMaterial(sphereMaterial);\nbox-&gt;setPos(glm::vec3(3.0, 0.0, 0.0));\n</code></pre>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#windows-and-gui","title":"Windows and GUI","text":"<p>To make it easy to create a window and GUI in the library, ivf2 comes with built-in support for GLFW and ImGui in the ivfui-library. Window support in ivf2 is implemented using the GLFW library encapsulated in GLFWApplication, GLFWWindow and GLFWSceneWindow classes. The Application class is a singleton class that manages GLFW windows and the rendering contexts. The following examples illustrates a versy simple ivf2 GLFW application:</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;vector&gt;\n\n#include &lt;ivfui/glfw_application.h&gt;\n#include &lt;ivfui/glfw_window.h&gt;\n\n#include &lt;ivf/gl.h&gt;\n#include &lt;ivf/nodes.h&gt;\n#include &lt;ivfui/ui.h&gt;\n\nusing namespace ivf;\nusing namespace ivfui;\nusing namespace std;\n\nclass ExampleWindow : public GLFWWindow {\nprivate:\n    CompositeNodePtr m_scene;\n    CameraManipulatorPtr m_camManip;\n\npublic:\n    ExampleWindow(int width, int height, std::string title) : GLFWWindow(width, height, title)\n    {}\n\n    static std::shared_ptr&lt;ExampleWindow&gt; create(int width, int height, std::string title)\n    {\n        return std::make_shared&lt;ExampleWindow&gt;(width, height, title);\n    }\n\n    int onSetup()\n    {\n        // Setup the scene\n\n        auto shaderMgr = ShaderManager::create();\n        shaderMgr-&gt;loadBasicShader();\n\n        auto lightMgr = LightManager::create();\n\n        auto pointLight1 = lightMgr-&gt;addPointLight();\n        pointLight1-&gt;setEnabled(true);\n        pointLight1-&gt;setDiffuseColor(glm::vec3(1.0, 1.0, 1.0));\n        pointLight1-&gt;setSpecularColor(glm::vec3(1.0, 1.0, 1.0));\n        pointLight1-&gt;setAttenuation(1.0, 0.0, 0.0);\n        pointLight1-&gt;setPosition(glm::vec3(5.0, 5.0, 5.0));\n        lightMgr-&gt;apply();\n\n        m_scene = CompositeNode::create();\n\n        auto axis = Axis::create();\n\n        m_scene-&gt;add(axis);\n\n        // Create more nodes ...\n\n        m_scene-&gt;add(node);\n\n        m_camManip = CameraManipulator::create(this-&gt;ref());\n\n        return 0;\n    }\n\n    void onDraw()\n    {\n        // This method is called continuously to draw the scene\n\n        glClearColor(0.07f, 0.13f, 0.17f, 1.0f);\n        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n\n        // Update the camera manipulator\n\n        m_camManip-&gt;update();\n\n        // Draw the scene\n\n        m_scene-&gt;draw();\n    }\n\n    void onResize(int width, int height)\n    {\n        // This method is called when the window is resized\n\n        m_camManip-&gt;update();\n    }\n};\n\ntypedef std::shared_ptr&lt;ExampleWindow&gt; ExampleWindowPtr;\n\nint main()\n{\n    // Create application object\n\n    auto app = GLFWApplication::create();\n\n    // Set context hints\n\n    app-&gt;hint(GLFW_CONTEXT_VERSION_MAJOR, 3);\n    app-&gt;hint(GLFW_CONTEXT_VERSION_MINOR, 3);\n    app-&gt;hint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);\n    app-&gt;hint(GLFW_SAMPLES, 4);\n\n    // Create a window\n\n    auto window = ExampleWindow::create(800, 800, \"Example 1\");\n    window-&gt;maximize();\n\n    // Add the window to the application\n\n    app-&gt;addWindow(window);\n\n    // Run the application\n\n    return app-&gt;loop();\n}\n</code></pre> <p>The final rendered window could look something like this:</p> <p></p>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#how-can-you-try-ivf2","title":"How can you try ivf2?","text":"<p>The library is still in development and not yet ready for release. However, you can try the library by cloning the repository from https://github.com/jonaslindemann/ivf2. Build instruction are available in the README.md file in the repository.</p>"},{"location":"blog/2025/01/12/the-fun-of-coding-3d-graphics-in-c-and-opengl/#getting-involved","title":"Getting involved","text":"<p>If you are interested in the project and want to get involved, my contact information is here: Jonas Lindemann</p>"},{"location":"blog/2024/07/19/a-new-backend-for-forcepad/","title":"A new backend for ForcePAD","text":"<p>The current version of ForcePAD is implemented using the FLTK library and OpenGL for graphics. At the time FLTK provided an easy way of implementing an multiplatform application that could run on Windows and Unix/Linux. FLTK provided both an intuitive environment for designing the 2D user interface as well as easy OpenGL context creation. OpenGL context creation on Windows and Unix was implemented very differently on each platform. Having just one way of this enabled ForcePAD to be implemented in a single code base.</p> <p>Using FLTK and OpenGL today has been more problematic. Even though context creation in FLTK is still easy. It is not very flexible preventing new modern OpenGL features from being easily implemented. Also, OpenGL today is a fragmented environment. Apple and macOS are currently not supporting the library and put restrictions on which versions they make available. At this point, I was a bit depressed about the lack of standards in graphics APIs. After I saw a YouTube video on game engines I was impressed with the game library RayLib (https://www.raylib.com/. </p> <p></p> <p>Raylib is a modular game engine that is available on all platforms. When compared to other game engines, Raylib is more of a toolbox where you can use what you need. This is very much the same experience I had with OpenGL a long time ago. The API provides both low-level functions as well as more high-level functions. An example of a very simple RayLib application is shown below:</p> <pre><code>#include \"raylib.h\"\n\nint main() \n{\n    // Initialization\n\n    int screenWidth = 800;\n    int screenHeight = 450;\n\n    InitWindow(screenWidth, screenHeight, \"Simple RayLib Example\");\n\n    SetTargetFPS(60); // Set our game to run at 60 frames-per-second\n\n    // Main game loop\n\n    while (!WindowShouldClose()) \n    {\n        // Draw\n\n        BeginDrawing();\n        ClearBackground(RAYWHITE);\n        DrawText(\"This is RayLib\", 190, 200, 20, LIGHTGRAY);\n        EndDrawing();\n    }\n\n    // De-Initialization\n\n    CloseWindow(); \n\n    return 0;\n}\n</code></pre> <p>This application will compile for any supported platform, which currently are:</p> <ul> <li>Windows</li> <li>Linux</li> <li>macOS</li> <li>iOS</li> <li>Android</li> <li>Raspberry Pi</li> <li>HTML5</li> </ul> <p>This also means that from the same source code, I can build a native application as well as an application that can be run directly on a web page using web assembly.</p> <p>In the following blog posts, I will try to describe how I set up the basic project and implement the basic abstractions for the application.</p> <p>Until the next blog post...</p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/","title":"Experimenting with AI and Mechanics applications","text":"<p>When working on the next version of ObjectiveFrame with improved scripting features, I got the idea to explore how well an AI agent would create a mechanical structure using the built-in scripting language (ChaiScript) in ObjectiveFrame. I had some initial sessions with Claude which confirmed that it was possible to create scripts from textual prompts and run them in the ObjectiveFrame's new scripting environment. This led me to my next idea of adding an interface to an AI agent directly in the ObjectiveFrame application. The AI agent would then be able to create mechanical structures based on the user's prompt. In the following sections, I will describe my process to implement this feature.</p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#first-conversations-with-claude","title":"First conversations with Claude","text":"<p>To test my idea of generating structures from prompts, I provided Claude with the following prompt:</p> <p>Question</p> <p>I have an 3D beam/bar analysis application that provide the following functions for creating geometry (implemented in chaiscript):</p> <pre><code>vfem::Node *addNode(double x, double y, double z);\nvfem::Beam *addBeam(int i0, int i1);\nsize_t nodeCount();\nvoid nodePos(vfem::Node *node, double &amp;x, double &amp;y, double &amp;z);\nvoid nodePosAt(int i, double &amp;x, double &amp;y, double &amp;z);\nvoid updateNodePos(vfem::Node *node, double x, double y, double z);\nvoid updateNodePosAt(int i, double x, double y, double z);\nvfem::Node *nodeAt(int i);\nbool isNodeSelected(vfem::Node *node);\nbool isNodeSelectedAt(int i);\ndouble randFloat(double min, double max);\nint randInt(int min, int max);\nvoid randSeed();\n</code></pre> <p>Using this information can you create a chaiscript from the following prompt:</p> <p>Create a beam structure for a building 10 x 10 m with a angled roof. Add stabilisation structures as well.</p> <p>In the prompt, I described my scripting interface and its functions. I also told Claude that I was using the scripting language Chaiscript. I then asked Claude to generate a script to create a beam structure for a building with a 10 x 10 m base and an angled roof. I also asked Claude to add stabilization structures to the building. The response I got was the following:</p> <p>Answer</p> <p>I'll create a ChaiScript that generates a beam structure for a 10m x 10m building with an angled roof and stabilization structures.Building Structure with Angled Roof and StabilizationClick to open codeTap to openThe script I've created generates a 10m x 10m building with an angled roof and comprehensive stabilization structures. Here's how it works:</p> <p>Building Dimensions:</p> <p>10m x 10m footprint 4m wall height 6m peak roof height (2m above the walls)</p> <p>Main Structure:</p> <p>Creates a rectangular base with four corner columns Forms a simple gable roof with a central ridge beam Includes top wall beams connecting the columns</p> <p>Stabilization Elements:</p> <p>X-bracing on all four walls for lateral stability Additional midpoint supports along each wall Ridge beam with supporting connections to all walls Purlins (horizontal roof supports) to reinforce the roof structure</p> <p>Node Arrangement:</p> <p>Ground corners are placed in a counter-clockwise pattern Wall tops align with the ground corners Roof peak forms along a central ridge line</p> <p>The script uses your provided ChaiScript functions to create nodes at specific coordinates and connect them with beams. It includes helper functions to simplify node creation and tracking. Would you like me to modify any aspects of this building structure? For example, I could adjust the roof angle, add more stabilization elements, or change the building proportions.</p> <p>To my big surprise, Claude actually generated a script that ran directly in ObjectiveFrame and created a beam structure for a building with a 10 x 10 m base and an angled roof. The script also added stabilization structures to the building. The code was well structured and used convenience functions to simplify the creation of nodes and beams. Some of the code is shown below:</p> <pre><code>// Building Structure Generator\n// Creates a 10m x 10m building with angled roof and stabilization elements\n\n// Define building dimensions\nvar width = 10.0;\nvar length = 10.0;\nvar wallHeight = 4.0;\nvar roofPeakHeight = 6.0;  // Peak of the roof is 2m above the wall tops\n\n// Function to create node and return its index\ndef createNodeAndGetIndex(x, y, z) {\n    var node = addNode(x, y, z);\n    return nodeCount() - 1;  // Return the index of the newly created node\n}\n\n// Create building nodes - ground floor corners (counter-clockwise from origin)\nprint(\"Creating building structure...\");\n\n// Ground floor corners\nvar n0 = createNodeAndGetIndex(0.0, 0.0, 0.0);         // Front left\nvar n1 = createNodeAndGetIndex(width, 0.0, 0.0);       // Front right\nvar n2 = createNodeAndGetIndex(width, length, 0.0);    // Back right\nvar n3 = createNodeAndGetIndex(0.0, length, 0.0);      // Back left\n\n// Top of walls (directly above ground floor corners)\nvar n4 = createNodeAndGetIndex(0.0, 0.0, wallHeight);         // Front left top\nvar n5 = createNodeAndGetIndex(width, 0.0, wallHeight);       // Front right top\nvar n6 = createNodeAndGetIndex(width, length, wallHeight);    // Back right top\nvar n7 = createNodeAndGetIndex(0.0, length, wallHeight);      // Back left top\n\n// Roof peak nodes\nvar n8 = createNodeAndGetIndex(width/2, 0.0, roofPeakHeight);     // Front center peak\nvar n9 = createNodeAndGetIndex(width/2, length, roofPeakHeight);  // Back center peak\n\n// Create center ridge beam\nvar ridge = createNodeAndGetIndex(width/2, length/2, roofPeakHeight);\n\n// Create vertical beams (columns)\nprint(\"Creating columns...\");\naddBeam(n0, n4);  // Front left column\naddBeam(n1, n5);  // Front right column\naddBeam(n2, n6);  // Back right column\naddBeam(n3, n7);  // Back left column\n\n// Create ground floor beams\nprint(\"Creating ground floor frame...\");\naddBeam(n0, n1);  // Front ground beam\naddBeam(n1, n2);  // Right ground beam\naddBeam(n2, n3);  // Back ground beam\naddBeam(n3, n0);  // Left ground beam\n\n// ... more code ...\n</code></pre> <p>Running the code in ObjectiveFrame created the following structure:</p> <p></p> <p>Obviously, the structure was created in the XY plane, but the ground plane in ObjectiveFrame is the XZ plane. I told Claude I wanted the structure to be created in the XZ plane instead. Claude then generated a new script that created the structure in the XZ plane. The code was similar to the previous one but with the coordinates adjusted to the XZ plane. This short experiment showed that it was possible to generate scripts from textual prompts and run them in the ObjectiveFrame application. Perhaps it would be possible to integrate this directly into the application?</p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#implementing-the-ai-interface-in-objectiveframe","title":"Implementing the AI interface in ObjectiveFrame","text":"<p>To implement structure generation using AI, I need to make sure that the prompt only generates Chaiscript code and no other additional information. To accomplish this, I decided to implement the system prompt with a set of rules that the AI agent must follow, followed by the actual prompt from the user. After some testing, the rules that seemed to generate the best results were:</p> <pre><code>You are a structural engineering assistant specialized in generating optimized ChaiScript code for 3D beam/bar structures.\n\nYour task is to convert natural language descriptions into optimized ChaiScript code.\n\nThe code you generate should follow these guidelines:\n\n1. COORDINATE SYSTEM:\n   - XZ is the ground plane, Y is height\n   - Origin (0,0,0) is at center of structure base unless specified otherwise\n   - All dimensions are in meters\n   - Try centering the structure around the origin\n\n2. OPTIMIZATION PRINCIPLES:\n   - Minimize redundant elements\n   - Each node must serve a structural purpose\n   - Create triangulated structures where possible\n   - Avoid duplicate beams and unnecessary nodes\n\n3. STRUCTURE GENERATION:\n   - Generate nodes only at essential structural points\n   - Connect beams in minimal paths that ensure structural integrity\n   - Use functions to create repeating patterns rather than hardcoding elements\n\n4. AVAILABLE FUNCTIONS:\n   - void addNode(double x, double y, double z);\n   - void addBeam(int i0, int i1);\n   - size_t addNodeWithIdx(double x, double y, double z);\n   - size_t addBeamWithIdx(int i0, int i1);\n   - size_t nodeCount();\n   - size_t beamCount();\n   - void nodePosAt(int i, double &amp;x, double &amp;y, double &amp;z);\n   - void updateNodePosAt(int i, double x, double y, double z);\n   - void beamAt(int i, int &amp;i0, int &amp;i1);\n   - void updateBeamAt(int i, int i0, int i1);\n   - bool isNodeSelectedAt(int i);\n   - double randFloat(double min, double max);\n   - int randInt(int min, int max);\n   - void randSeed();\n   - bool isNodeSelectedAt(int i);\n   - void selectAllElements();\n   - void selectAllNodes();\n   - void meshSelectedNodes();\n   - void surfaceSelectedNodes(bool groundElements = true);\n   - newModel();\n\n5. APPLICATION INFORMATION\n   - Nodes are created consequentially, starting from 0. \n   - Beams are created by specifying the indices of the nodes they connect. 0 based.\n   - Node indices start from 0 and increase by 1 for each new node.\n   - Start models with newModel() to clear the workspace.\n   - Assume node ordering is invalid after meshSelectedNodes() or surfaceSelectedNodes().\n\n6. CHAISCRIPT TYPES:\n    - Vectors and Maps are used in the following way:\n\n        var v = [1,2,3u,4ll,\"16\", `+`]; // creates vector of heterogenous values\n        var m = [\"a\":1, \"b\":2]; // map of string:value pairs\n\n        // Add a value to the vector by value.\n        v.push_back(123);\n\n        // Add an object to the vector by reference.\n        v.push_back_ref(m);\n\n    - Declare variables required in functions with the global keyword instead of var.\n    - Functions are defined using the def keyword.\n    - The PI constant is not defined in ChaiScript, so you can define it as a global variable using the global keyword.\n    - All declared variables must be initialized with a value.\n    - When using integer divisions in floating point arithmetic, cast the numerator to a floating point number.\n\n7. OTHER CONSIDERATIONS:\n   - Avoid calling methods on vfem::Node or vfem::Beam objects directly. Use the provided functions instead.\n   - No need to store generated nodes in separate data structures. Generate them as needed.\n   - Provide only ChaiScript code with minimal comments explaining the structure. Do not include explanations outside the code.\n</code></pre> <p>As you can see, I give some initial ground rules on how I want the structure generated, followed by the available functions in ChaiScript. When experimenting with the system prompt, I also realized that the AI agent needed guidance on how to declare certain variables in the ChaiScript code. I, therefore, added a section on the specifics of the ChaiScript language. In the following section, I will go through how I implemented the AI agent in ObjectiveFrame.</p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#a-structure-generation-class","title":"A Structure Generation Class","text":"<p>To implement the AI agent in ObjectiveFrame, I created a new class called <code>StructureGenerator</code>. The class is responsible for generating ChaiScript code from a user prompt. The class has the following structure:</p> <pre><code>/**\n * A class that interfaces with the Claude API to generate optimized ChaiScript code\n * for structural modeling based on natural language prompts.\n */\nclass StructureGenerator {\npublic:\n    // Define a callback type for asynchronous operations\n    using GenerationCallback = std::function&lt;void(const std::string &amp;, bool)&gt;;\n\nprivate:\n    std::string m_apiKey;\n    std::string m_apiUrl;\n    std::string m_model;\n    std::string m_systemPrompt;\n\n    static size_t WriteCallback(void *contents, size_t size, size_t nmemb, std::string *output);\n    std::string buildSystemPrompt() const;\n    std::string makeClaudeRequest(const std::string &amp;userPrompt);\n    std::string extractChaiScript(const std::string &amp;claudeResponse);\n\npublic:\n    StructureGenerator(const std::string &amp;apiKey);\n    ~StructureGenerator();\n\n    void setModel(const std::string &amp;modelName);\n\n    void setApiKey(const std::string &amp;apiKey);\n    void loadSystemPromptFrom(const std::string &amp;filename);\n\n    std::string generateStructure(const std::string &amp;prompt);\n    void generateStructureAsync(const std::string &amp;prompt, GenerationCallback callback);\n    std::future&lt;std::string&gt; generateStructureAsync(const std::string &amp;prompt);\n\n};\n</code></pre> <p>I am using the Claude API as the backend AI agent for this application. Accessing the Claude API requires an API key, which can be acquired from Anthropic's console service. The user must provide their API key to use ObjectiveFrame with the Claude API.</p> <p>Using the class can be done either blocking or non-blocking. The easiest way is to use the blocking call as shown in the code below:</p> <pre><code>// Initialize the generator with your API key\nStructureGenerator generator(\"your-api-key-here\");\n\n// Generate ChaiScript code for a structure\nstd::string chaiScript = generator.generateStructure(\n    \"Create a building structure 10m \u00d7 10m with 4m wall height and an angled roof peaking at 6m. \"\n    \"Add stabilization with single diagonals on each wall and direct roof supports.\");\n\n// Print or use the generated code\nstd::cout &lt;&lt; chaiScript &lt;&lt; std::endl;\n</code></pre> <p>This produces the following output:</p> <pre><code>// Building structure 10m \u251c\u00f9 10m with 4m wall height and angled roof peaking at 6m\nnewModel();\n\n// Define building dimensions\nglobal building_width = 10.0;\nglobal building_depth = 10.0;\nglobal wall_height = 4.0;\nglobal peak_height = 6.0;\n\n// Create base nodes (corners at ground level)\nglobal n0 = addNodeWithIdx(-building_width/2, 0, -building_depth/2); // front left\nglobal n1 = addNodeWithIdx( building_width/2, 0, -building_depth/2); // front right\nglobal n2 = addNodeWithIdx( building_width/2, 0,  building_depth/2); // back right\nglobal n3 = addNodeWithIdx(-building_width/2, 0,  building_depth/2); // back left\n\n// Create wall top nodes\nglobal n4 = addNodeWithIdx(-building_width/2, wall_height, -building_depth/2); // front left top\nglobal n5 = addNodeWithIdx( building_width/2, wall_height, -building_depth/2); // front right top\nglobal n6 = addNodeWithIdx( building_width/2, wall_height,  building_depth/2); // back right top\nglobal n7 = addNodeWithIdx(-building_width/2, wall_height,  building_depth/2); // back left top\n\n// Create roof peak nodes (in the middle of the building)\nglobal n8 = addNodeWithIdx(0, peak_height, -building_depth/2); // front peak\nglobal n9 = addNodeWithIdx(0, peak_height,  building_depth/2); // back peak\n\n// Base frame\naddBeam(n0, n1); // front base\naddBeam(n1, n2); // right base\naddBeam(n2, n3); // back base\naddBeam(n3, n0); // left base\n\n// Vertical columns\naddBeam(n0, n4); // front left column\naddBeam(n1, n5); // front right column\naddBeam(n2, n6); // back right column\naddBeam(n3, n7); // back left column\n\n// Wall top frame\naddBeam(n4, n5); // front top\naddBeam(n5, n6); // right top\naddBeam(n6, n7); // back top\naddBeam(n7, n4); // left top\n\n// Roof structure\naddBeam(n4, n8); // front left to front peak\naddBeam(n5, n8); // front right to front peak\naddBeam(n7, n9); // back left to back peak\naddBeam(n6, n9); // back right to back peak\naddBeam(n8, n9); // peak connection beam\n\n// Wall diagonals for stabilization\naddBeam(n0, n5); // front wall diagonal\naddBeam(n1, n6); // right wall diagonal\naddBeam(n2, n7); // back wall diagonal\naddBeam(n3, n4); // left wall diagonal\n\n// Direct roof supports\naddBeam(n4, n6); // diagonal cross brace 1\naddBeam(n5, n7); // diagonal cross brace 2\n</code></pre> <p>This operation is relatively long-running and ObjectiveFrame is an immediate rendering model application, so we can't have a blocking call in the rendering loop. Therefore, I also implemented an asynchronous version of the function that can be used in the rendering loop. This method takes a callback function as an argument and calls this function when structure generation has completed. This allows for it to be integrated into the rendering loop. The code below shows how the generation is integrated as a method in the FemView class of ObjectiveFrame:</p> <pre><code>void FemViewWindow::makeAiRequest(const std::string &amp;userPrompt)\n{\n    if (m_aiApiKey.empty())\n    {\n        log(\"No API key set for AI service.\");\n        return;\n    }\n\n    m_structureGenerator.setApiKey(m_aiApiKey);\n    m_structureGenerator.generateStructureAsync(userPrompt, std::bind(&amp;FemViewWindow::onGenerationComplete, this,                                        std::placeholders::_1, std::placeholders::_2));\n\n    m_isProcessingAiRequest = true;\n}\n</code></pre> <p>The internal implementation of the StructureGenerator class is based on the cURL library and the nlohmann/json library for calling the Claude API and processing the response. </p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#implementing-a-gui-for-the-ai-agent","title":"Implementing a GUI for the AI agent","text":"<p>The main idea for the user interface is to extend the Create menu in ObjectiveFrame with a Create using AI item. This option will display a prompt window where the user can enter the prompt and request a structure. The window is divided into 2 sections: the first is the prompt text field, and the second displays the generated code. This lets the user copy the code to the script editor for further modifications. The prompt window is shown below:</p> <p></p> <p>The user interface has 5 controls:</p> <ul> <li>Clear prompt: Clears the prompt text field.</li> <li>Clear model: Clears the model in the rendering window.</li> <li>Generate: Generates the structure based on the prompt.</li> <li>Auto run: When checked will automatically run the generated code (default). </li> <li>Run: Runs the code in the response window.</li> </ul> <p>The check box was added to allow the user to decide whether the generated code should be run automatically or not. This is useful if the user wants to modify the code before running it. </p> <p>When the user selects generate, the user interface shows a progress bar, and the prompt text field is hidden, as shown below:</p> <p></p> <p>When the generation is complete the response window is shown with the generated code as in the following image:</p> <p></p> <p>The generation is usually successful, but sometimes, the AI agent generates strange structures from the prompt. Most of the time, it is enough for the user to run the prompt again. Error messages from code generation are shown in the ObjectiveFrame log window.</p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#implementing-the-ui-in-objectiveframe","title":"Implementing the UI in ObjectiveFrame","text":"<p>The user interface is implemented in two parts. The first part is the ofui::PromptWindow class, which renders the prompt window and its controls using ImGui. The second part includes additional methods in the FemViewWindow class, which is responsible for rendering the main window and the view of the model. </p> <p>First, we show the prompt window in the onDrawImGui method of the FemViewWindow class:</p> <pre><code>    // ...\n\n    if (ImGui::BeginMenu(\"Create\"))\n    {\n        if (ImGui::MenuItem(\"Create using AI\", \"\"))\n            m_promptWindow-&gt;show();\n\n    // ...\n</code></pre> <p>The ofui::PromptWindow will call the FemView class when the user selects the Generate button. The FemView class will then call the makeAiRequest method to generate the structure. The code below shows the implementation of the onGenerationComplete method in the FemView class:</p> <pre><code>    // ...\n\n    if (ImGui::Button(\"Generate\", ImVec2(contentSize.x * 0.2, 0)))\n    {\n        if (m_view)\n        {\n            clearOutput();\n            m_view-&gt;makeAiRequest(prompt());\n        }\n    }\n</code></pre> <p>As makeAiReques(...) is non-blocking, there is no problem calling this method in the ImGui rendering loop. The onGenerationComplete method is called when the structure generation is complete. The code below shows the implementation of the onGenerationComplete method in the FemView class:</p> <pre><code>void FemViewWindow::onGenerationComplete(const std::string &amp;result, bool success)\n{\n    if (success)\n    {\n        log(\"AI generation successful.\");\n        m_promptWindow-&gt;clearOutput();\n        m_promptWindow-&gt;addOutput(result);\n        if (m_autoRunAiScript)\n            this-&gt;runScriptFromText(result);\n        m_isProcessingAiRequest = false;\n    }\n    else\n    {\n        log(\"AI generation failed.\");\n    }\n    m_isProcessingAiRequest = false;\n}\n</code></pre>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#conclusion","title":"Conclusion","text":"<p>This small experiment has shown that it is feasible to implement an AI agent interface in an existing application. One of the key elements making this possible is that the application has some kind of built in scripting language such as ChaiScript or Python. By generating a suitable system prompt it is possible for the application to send a prompt from the user to a backend AI interface and then execute the received script code in the application. There is still a lot of work to fine tune the system prompt to improve the structure generation as it currently stands is not perfect (more on the level of a beginner in structural engineering). However, it can be developed to become a future tool for assisting architects and engineers in the early stages of the design process.</p> <p>A demo of the AI agent in ObjectiveFrame can be seen in the video below:</p>"},{"location":"blog/2025/03/23/experimenting-with-ai-and-mechanics-applications/#links","title":"Links","text":"<ul> <li>ObjectiveFrame</li> <li>ObjectiveFrame download for Windows</li> <li>ObjectiveFrame on GitHub</li> <li>ChaiScript</li> <li>Claude API</li> <li>cURL</li> <li>nlohmann/json</li> </ul>"},{"location":"blog/2024/08/14/building-for-many-platforms-is-hard/","title":"Building for many platforms is hard","text":"<p>Choosing the right libraries for your application is just one step in making it run on multiple operating systems. ForcePAD uses CMake to build the application on multiple platforms. CMake is an excellent tool that generates build files for each platform. On Windows it generates Visual Studio solution files. On other platform it generates make files. I started implementing most of the tooling for ForcePAD on Windows thinking that most of this can be easily transferred to macOS. This what not that easy. I will go through the setup in the following sections.</p>"},{"location":"blog/2024/08/14/building-for-many-platforms-is-hard/#windows-and-dependencies","title":"Windows and dependencies","text":"<p>The biggest issue with building applications on Windows is the dependencies that on many other operating systems such as Linux are easily installable using a package manager. Until recently there was no real packages manager available on Windows, requiring you to download, build and install dependencies manually. Today there are several package managers for C++ that can solve this problem, such as Conan and Vcpkg. For ForcePAD I chose to use Vcpkg as it has worked well for other projects. To use Vcpkg you provide a vcpkg.json file in your root build directory. In this file you specify the dependencies you have in your project. An example of this is shown below:</p> <pre><code>{\n    \"name\": \"forcepad\",\n    \"version\": \"0\",\n    \"dependencies\":\n    [\n        {\n            \"name\": \"raylib\",\n            \"platform\": \"windows\"\n        },\n        {\n            \"name\": \"opengl\",\n            \"platform\": \"windows\"\n        },    \n        {\n            \"name\": \"eigen3\",\n            \"platform\": \"windows\"\n        },\n        \"stb\",\n        {\n            \"name\": \"imgui\",\n            \"features\": [\"docking-experimental\"]\n        },\n        {\n            \"name\": \"glfw3\",\n            \"platform\": \"windows\"\n        }\n    ]\n}\n</code></pre> <p>The important stuff is contained in the dependencies section. Here you list the packages you require. The platform attributes indicates for which platform this dependency is required. It is optional, but I will come back to this later. To build the dependencies you type:</p> <pre><code>C:\\...\\&gt;vcpkg install\n</code></pre> <p>This will build the dependencies in your current directory. By default build libraries will be placed in the vcpkg_installed directory. </p> <p>To make sure these libraries are found by CMake we need to define a CMakePresets.json file where we specify where Vcpkg can be found for the different platforms. The preset file for ForcePAD is shown below:</p> <pre><code>{\n  \"version\": 3,\n  \"configurePresets\": [\n    {\n      \"name\": \"default\",\n      \"toolchainFile\": \"e:/vcpkg/scripts/buildsystems/vcpkg.cmake\"\n    },\n    {\n      \"name\": \"linux\",\n      \"toolchainFile\": \"/home/bmjl/vcpkg/scripts/buildsystems/vcpkg.cmake\"\n    },\n    {\n      \"name\": \"macos\",\n      \"toolchainFile\": \"/Users/lindemann/vcpkg/scripts/buildsystems/vcpkg.cmake\"\n    }\n  ]\n}\n</code></pre>"},{"location":"blog/2024/08/14/building-for-many-platforms-is-hard/#a-cmakeliststxt-file-for-all-platforms","title":"A CMakeLists.txt file for all platforms","text":"<p>The nice thing using a package manager is that the actual CMakeLists.txt file doesn't have to have explicit paths for different libraries. Normal find_package(...) commands can be called and corresponding paths will be found. The commands used to find the packages are the following:</p> <pre><code>find_package(raylib CONFIG REQUIRED)\nfind_package(Eigen3 CONFIG REQUIRED)\nfind_package(imgui CONFIG REQUIRED)\nfind_package(OpenGL REQUIRED)\nfind_package(Stb REQUIRED) \nfind_package(glfw3 CONFIG REQUIRED)\n</code></pre>"},{"location":"blog/2024/08/14/building-for-many-platforms-is-hard/#configuring-and-building-with-cmake","title":"Configuring and building with CMake","text":"<p>To configure a debug build of ForcePAD using the default preset CMake is called with the following command line:</p> <pre><code>C:\\...\\&gt;cmake -B build-debug -DCMAKE_BUILD_TYPE=Debug --preset default\n</code></pre> <p>This configures the ForcePAD debug build in the build-debug directory. Building ForcePAD can now be done using the following command:</p> <pre><code>C:\\...\\&gt;cmake --build build-debug --config Debug -- /m\n</code></pre> <p>It is of course possible to use Visual Studio solution files contained in the build-debug directory to edit and build ForcePAD.</p>"},{"location":"blog/2024/08/14/building-for-many-platforms-is-hard/#multiplatform-is-still-hard","title":"Multiplatform is still hard","text":"<p>At this point I was very happy to have build system that at least in theory should work on both Windows and macOS. I copied over all my source files to my newly aquired Mac mini. Installed all development tools and MacBrew. </p> <p>I ran the same commands and Vcpkg started to build the dependencies, but the compiler and linker errors where plenty. I realised that using the Apple provided g++ compiler was not a good choice, so I switched to the brew provided compilers g++-13/gcc-13. This required me to make sure that they where used instead of the Apple-compilers. Using the following statements I set the required environment variables so that CMake and Vcpkg picked up the right compilers:</p> <pre><code>export CXX=g++-13\nexport CC=gcc-13\n</code></pre> <p>This reduced some of the compiler errors, but I still got a lot of linker errors with Vcpkg build libraries. Then I had an idea. Perhaps I should use brew to provide some of the packages instead. I added the platform directive in the vcpkg.json file and I told vcpkg only to build raylib, opengl, eigen3 and glfw3 on Windows. This make a lot of errors go away. However I got stuck on missing system libraries from Apple such as CoreGraphics and Cocoa. I modified my CMakeLists.txt for the main ForcePAD executable to the following:</p> <pre><code>if (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n    if (APPLE)\n        add_executable(forcepad3d main.cpp forcepad_window.cpp forcepad_window.h)\n        target_include_directories(forcepad3d PRIVATE ${gui_INCLUDE_DIRS} ${raylib_INCLUDE_DIRS})\n        target_link_libraries(forcepad3d PRIVATE \"-framework CoreGraphics\" \"-framework Cocoa\" \"-framework IOKit\" \"-framework OpenGL\" \"-framework CoreFoundation\" rlimguid guid graphicsd ${raylib_LIBRARIES} OpenGL::GL imgui::imgui glfw)\n    else()\n        ...\n</code></pre> <p>These changes made ForcePAD build on macOS and worked on first startup. To make configuration a bit I created a build.cmd on Windows and a build.sh on macOS to automate some of the steps.</p>"},{"location":"blog/2024/08/14/building-for-many-platforms-is-hard/#conclusions","title":"Conclusions","text":"<p>Building a multiplatform application is hard. Raylib made it a bit easier on the source level, but still requires dependencies on the system side. It also worries me a bit that Raylib relies on OpenGL, which is kind of unsupported on macOS, but I suspect the Raylib developers could do some port of the rlgl to metal or other libraries.</p>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/","title":"CALFEM for Python 3.6.10 released","text":"<p>CALFEM for Python is a comprehensive package designed for learning the finite element method. Originally developed in the 1980s using Fortran, CALFEM has since evolved to support MATLAB and Python. The Python version was specifically created to facilitate the course \"Software Development for Technical Applications\" at Structural Mechanics in Lund. Throughout the course, students gain hands-on experience in developing a complete finite element application in Python. They learn how to implement various components, including mesh generation, solver, user interface, and visualization routines.</p> <p>In this blog post, I will provide a detailed explanation of the packaging and installation process for CALFEM.</p>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#distributing-and-packaging-calfem-for-python","title":"Distributing and packaging CALFEM for Python","text":"<p>The early versions of CALFEM for Python were distributed by placing all the Python source code files in a zip-archive and linking it on Structural Mechanics web pages. However, this approach proved to be cumbersome. In 2009, we transitioned to using Subversion as a source repository for better development tracking. In recent years, we have migrated all our development efforts to GitHub.</p> <p>Using a source repository allowed us to tag released versions and provide links to the latest version of CALFEM on our web pages. However, even with this improvement, the process of installing the package and setting the PYTHONPATH still required manual installation of all the dependencies. To address this issue, I modified the repository to create a package called calfem-python using setuptools in Python. This involved creating a special setup.py script in the source directory, which is used to generate the packages. The generated tar files are then uploaded to PyPI using specific commands.</p>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#creating-packages-with-pdm","title":"Creating packages with PDM","text":"<p>Releasing packages in the traditional way can be unnecessarily complicated. Fortunately, there are several packaging tools available that simplify the package creation process. One such tool is PDM (Python Package and Dependency Manager), which I recently discovered and found to be incredibly user-friendly.</p> <p>To utilize PDM, you'll need to make some modifications to your source tree. In the case of the calfem-python package, all source modules were initially located in the calfem directory within the source tree. However, PDM requires all source files to be placed under the src directory, as demonstrated below.</p> <pre><code>\u251c\u2500\u2500\u2500src\n\u2502   \u2514\u2500\u2500\u2500calfem\n</code></pre> <p>To use PDM in your source tree you run pdm init the first time. This will ask you a number of questions and generate a pyproject.toml file. The pyproject.toml file of the calfem-python is shown below:</p> <p>First we define the main project attributes such as package-name and version.</p> <pre><code>[project]\nname = \"calfem-python\"\nversion = \"3.6.10\"\ndescription = \"CALFEM for Python\"\nauthors = [\n    {name = \"Jonas Lindemann\", email = \"jonas.lindemann@lunarc.lu.se\"},\n    {name = \"Jonas Lindemann\", email = \"jonas.lindemann@gmail.com\"},\n]\n</code></pre> <p>Next we need to specify the hard dependencies of the package:</p> <pre><code>dependencies = [\n    \"gmsh\",\n    \"matplotlib\",\n    \"numpy\",\n    \"scipy\",\n    \"tabulate\",\n]\n\nrequires-python = \"&gt;=3.8\"\n</code></pre> <p>In the following part we specify some package metadata such as README files, license type and keywords.</p> <pre><code>readme = \"README.md\"\nlicense = {text = \"MIT\"}\nkeywords = [\n    \"finite element\",\n    \"math\",\n    \"numerics\",\n]\n</code></pre> <p>In the classifiers section we give some additional project metadata.</p> <pre><code>classifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Topic :: Software Development :: Build Tools\",\n]\n</code></pre> <p>We can also provide a linke to a project home page.</p> <pre><code>[project.urls]\nHomepage = \"https://github.com/CALFEM/calfem-python\"\n</code></pre> <p>The pyproject.toml file is a standardised build file used by many build systems. To specify that this package requires PDM we specify this in the build-system secion.</p> <pre><code>[build-system]\nrequires = [\"pdm-backend\"]\nbuild-backend = \"pdm.backend\"\n\n[tool.pdm]\ndistribution = true\n</code></pre> <p>Finally we list any optional dependencies.</p> <pre><code>[project.optional-dependencies]\nvisvis = [ \"visvis\" ]\nvedo = [ \"vedo\" ]\npyvtk = [ \"pyvtk\" ]\nqtpy = [ \"qtpy\" ]\n</code></pre>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#creating-the-package","title":"Creating the package","text":"<p>Dependencies are installed from a so called lock-file. The lock file is generated by PDM to define the absolute truth on which dependency packages to use. There are several commands to manage the lock-file. The most common in the pdm install command. This will determine required dependencies and update the lock-file.</p> <pre><code>C:\\&gt; pdm update\nDEPRECATED: `cross_platform` strategy is deprecated in favor of the new lock targets.\nSee docs: http://pdm-project.org/en/latest/usage/lock-targets/\nC:\\Users\\jonas\\miniconda3\\envs\\calfem-dev-3.12\\Lib\\site-packages\\pdm\\resolver\\providers.py:200: PackageWarning: Skipping\nmatplotlib@3.9.2 because it requires Python&gt;=3.9 but the lock targets to work with Python&gt;=3.8. Instead, another version\nof matplotlib that supports Python&gt;=3.8 will be used.\n...\n\u283c 0:00:04 Resolve for environment (&gt;=3.8) 17 resolved, 0 to resolveINFO: Use `-q/--quiet` to suppress these warnings, or ignore them per-package with `ignore_package_warnings` config in\n[tool.pdm] table.\n  0:00:06 \ud83d\udd12 Lock successful.\nSynchronizing working set with resolved packages: 0 to add, 2 to update, 0 to remove\n\n  \u2714 Update matplotlib 3.7.4 -&gt; 3.7.5 successful\n  \u2714 Update gmsh 4.12.2 -&gt; 4.13.1 successful\n  \u2714 Update calfem-python 3.6.10 -&gt; 3.6.10 successful\n\n  0:00:03 \ud83c\udf89 All complete! 2/2\n\nC:\\&gt; pdm install\nAll packages are synced to date, nothing to do.\n  \u2714 Update calfem-python 3.6.10 -&gt; 3.6.10 successful\n\n  0:00:00 \ud83c\udf89 All complete! 0/0\n</code></pre>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#building-the-package","title":"Building the package","text":"<p>To build the package files we use the pdm build command.</p> <pre><code>pdm build\nBuilding sdist...\nBuilt sdist at D:/Users/Jonas/Development/calfem-python/dist\\calfem_python-3.6.10.tar.gz\nBuilding wheel from sdist...\nBuilt wheel at D:/Users/Jonas/Development/calfem-python/dist\\calfem_python-3.6.10-py3-none-any.whl\n</code></pre> <p>This will build a source distribution calfem_python-3.6.10.tar.gz in the dist directory and a prebuilt module or Wheel, calfem_python-3.6.10-py3-none-any.whl, also in the dist directory.</p>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#publishing-the-package","title":"Publishing the package","text":"<p>When you have tested your package it is time to publish it in PyPI. This is done with the pdm publish command. This will upload the previously build distribution files.</p>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#installing-calfem-for-python","title":"Installing CALFEM for Python","text":"<p>To get the best environment for CALFEM we used the conda-forge Python distribution. First we create a conda environment for CALFEM.</p> <pre><code>(base) C:\\&gt; conda create -n calfem-env-3.12 python=3.12 numpy scipy matplotlib qtpy pyqt tabulate\n</code></pre> <p>This will install optimised versions of numpy and scipy. Next we activate our environment:</p> <pre><code>(base) C:\\&gt; conda activate calfem-env-3.12\n(calfem-env-3.12) C:\\&gt; \n</code></pre> <p>Now we are ready to install the published calfem-python package using pip.</p> <pre><code>(calfem-env-3.12) C:\\&gt; pip install calfem-python\n</code></pre> <p>Now we have a working CALFEM for Python environment for all of our finite element needs.</p>"},{"location":"blog/2025/01/25/coding-at-the-speed-of-light/","title":"Coding at the speed of light","text":"<p>As I have mentioned in my previous post, I have been working on my graphics library Ivf++ 2.0. There are many features I would like to implement, but as this is a hobby project I have limited time to work on it. My common approach has been to select a feature to implement, do research online, and then start coding. You start with great energy and enthusiasm, but you often get stuck on small things as you progress. Why doesn't this approach work? How does this function work? Why is this not rendering correctly? These are some of the questions that come up. You can spend hours figuring out the problem; sometimes, you can't find the solution. This can be very frustrating and demotivating.</p> <p>Some time ago, I started exploring AI tools. In 2022, I started experimenting with ChatGPT, first by letting it suggest improvements to existing codes in Python. I was amazed at the improvements it suggested. I also used it to answer questions on different programming topics. One thing that impressed me was the ability of the language model to translate numerical code written in Python and Numpy to C++ and Eigen. Worked almost perfectly. Later, during 2023, I used it in the refactoring of ObjectiveFrame. It helped me to refactor the codebase and make it more readable.</p> <p>Another AI tool that I started using as an early adopter was Github Copilot from Microsoft. It is a code completion that uses a language model to provide the completions. For me, it has made it fun to code again. It's almost like having a pair of programmers who know everything and help out with the boring parts of coding. By combining classical chat-based AI with Copilot, I have implemented features in Ivf++ 2.0 at a pace that I would not have been able to do otherwise. It feels like coding at the speed of light. In the rest of this post, I will describe some examples of how I have used these tools in the development of Ivf++ 2.0.</p>"},{"location":"blog/2025/01/25/coding-at-the-speed-of-light/#implementing-selection-rendering","title":"Implementing selection rendering","text":"<p>One of the features I wanted to implement was selection rendering. This feature allows you to select objects in the scene by clicking on them. I used buffer selection in my previous implementation of Ivf++ in OpenGL 1.1. This is a deprecated feature in OpenGL, and I wanted to implement something that would work in modern OpenGL. I have read earlier that this could be accomplished using shaders. The approach is to render all objects in the scene with a unique color and then read the pixel color under the mouse cursor. This color can then be used to identify the object that was clicked on. When it comes to shaders, I am still a beginner, so I decided to ask Claude for help. </p> <p>First, I asked a high-level question on different approaches to implementing selection rendering in modern OpenGL.</p> <p>Question</p> <p>I have implemented a scene graph library i C++/OpenGL supporting hierarchical object structures. I would like to be able to interact with the rendered objects. What is the best approach to implement object selection?</p> <p>The answer I got gave an overview of the different methods of implementing selection rendering in modern OpenGL. The answer I got in short form was:</p> <p>Answer</p> <p>For implementing object selection in a scene graph with OpenGL, there are several common approaches, each with their own trade-offs. Let me explain the main techniques:</p> <p>Ray Casting with Bounding Volumes:</p> <p>[ description of the method and code examples ]</p> <p>Color Picking:</p> <p>[ description of the method and code examples ]</p> <p>GPU picking using Compute shaders</p> <p>[ description of the method and code examples ]</p> <p>Every approach was provided with a code example. I decided to go with the color picking approach. Then I asked the follow up question:</p> <p>Question</p> <p>Ok, the buffer selection seems to be the quickest way forward. Please suggest suitable shaders and implementation for buffer selection.</p> <p>It then answered:</p> <p>Answer</p> <p>I'll help you implement color buffer selection, which is indeed a straightforward approach. Here's a complete implementation:</p> <p>[ Implementing a color buffer in C++/OpenGL ] </p> <p>Vertex shader:</p> <p>[ GLSL verted shader ]</p> <p>Fragment shader:</p> <p>[ GLSL fragment shader ]</p> <p>Integration of the shaders in the rendering pipeline:</p> <p>[ C++ code implementing in scene graph ]</p> <p>The suggested shaders were a bit different from the shaders I hade implemented in Ivf++, so I asked Claude to help me modify them to fit my implementation. </p> <p>Question</p> <p>I want to use the same structure as my standard shader, would this work as a fragment shader?</p> <pre><code>#version 330 core\nin vec3 normal;  \nin vec3 fragPos;  \nin vec4 color;\nin vec2 texCoord;\nuniform uint objectId;\nout vec4 fragColor;\nvoid main()\n{\n    fragColor = vec4(float(objectId &amp; 0xFF) / 255.0, float((objectId &gt;&gt; 8) &amp; 0xFF) / 255.0, float((objectId &gt;&gt; 16) &amp; 0xFF) / 255.0, 1.0);\n}\n</code></pre> <p>The answer was almost like having a conversation with an OpenGL expert. It also gave me the code for decoding the value when reading back from the selection buffer. I have to buy Claude a beer next time I meet him.</p> <p>Answer</p> <p>Yes, your approach using <code>vec4</code> as output and encoding the objectId into RGB components is a valid alternative to my suggestion! This is actually a more traditional approach and has some advantages:</p> <ol> <li>It's compatible with standard framebuffer formats (no need for R32UI)</li> <li>Works with older OpenGL versions</li> <li>Reuses your existing vertex shader and interpolated attributes</li> </ol> <p>The key differences: - Your shader packs the ID into RGB channels (24-bit ID space = 16,777,216 unique IDs) - My version used R32UI (32-bit ID space = 4,294,967,296 unique IDs)</p> <p>When reading the selection buffer, you'll need to decode the ID:</p> <pre><code>uint32_t readPixel(int x, int y) {\n    unsigned char pixel[3];\n    glReadPixels(x, y, 1, 1, GL_RGB, GL_UNSIGNED_BYTE, pixel);\n    return pixel[0] | (pixel[1] &lt;&lt; 8) | (pixel[2] &lt;&lt; 16);\n}\n</code></pre> <p>This approach will work perfectly fine for most scene graphs. The only limitation would be if you need more than 16.7 million unique objects, which is rarely the case.</p> <p>Would you like me to show how to integrate this with the rest of the selection buffer code?</p> <p>Now, I have what I need to start implementing the selection process in Ivf++. However, during this process, I needed a way to debug how the library was rendering the objects with unique identifiers, so I asked Claude for help with this as well.</p> <p>Question</p> <p>What would be a good way of debugging this?</p> <p>The answer was to use a debug shader that would render the object with the unique color. It also provided me with the code for the debug shader. Really helpful.</p> <pre><code>#version 330 core\nin vec3 normal;  \nin vec3 fragPos;  \nin vec4 color;\nin vec2 texCoord;\nuniform uint objectId;\nuniform bool debugMode;\nout vec4 fragColor;\n\nvoid main()\n{\n    vec4 idColor = vec4(\n        float(objectId &amp; 0xFF) / 255.0, \n        float((objectId &gt;&gt; 8) &amp; 0xFF) / 255.0, \n        float((objectId &gt;&gt; 16) &amp; 0xFF) / 255.0, \n        1.0\n    );\n\n    // In debug mode, make colors more visible\n    if (debugMode) {\n        fragColor = idColor * 0.8 + vec4(0.2);  // Brighten for visibility\n    } else {\n        fragColor = idColor;\n    }\n}\n</code></pre>"},{"location":"blog/2025/01/25/coding-at-the-speed-of-light/#implementing-a-spline-interpolation-class","title":"Implementing a Spline interpolation class","text":"<p>Another feature I wanted to implement was a spline interpolation class. I had implemented this before in Ivf++ 1.0, but I wanted to improve the implementation. After getting some experience in writing prompts, I know now that it is essential to be very specific and provide as many details as possible when asking the AI for help. So, to get the code I want, I give it clear instructions, such as dividing up the class in a header and a source file, what prefix to use for the class attributes, and how I want the methods to work. The final prompt became:</p> <p>Question</p> <p>I would like to have a Spline class in C++ where you can add points. All the points should be included in the spline. t = 0.0 -&gt; first point. t = n - 1 is the last point. Positions on the spline should be retrieved by either specifying the t or the actual geometric distance from the first point. Use glm for the points. member attributes should use m_ as prefix. Separate include and source files. Use pragma once for header guard.</p> <p>The anwser was almot perfect. I only had to make some minor adjustments in the final code. </p> <pre><code>// Spline.h\n#pragma once\n#include &lt;vector&gt;\n#include &lt;glm/glm.hpp&gt;\n\nclass Spline {\npublic:\n    // Add a point to the spline\n    void addPoint(const glm::vec3&amp; point);\n\n    // Get position on spline using parameter t\n    // t ranges from 0 to (number of points - 1)\n    glm::vec3 getPositionByT(float t) const;\n\n    // Get position on spline using distance from start\n    glm::vec3 getPositionByDistance(float distance) const;\n\n    // Get total length of the spline\n    float getTotalLength() const;\n\n    // Get number of points in the spline\n    size_t getPointCount() const;\n\n    // Clear all points\n    void clear();\n\nprivate:\n    std::vector&lt;glm::vec3&gt; m_points;\n    std::vector&lt;float&gt; m_distances; // Cumulative distances from start\n\n    // Recalculate cumulative distances when points change\n    void updateDistances();\n\n    // Convert distance to t parameter\n    float distanceToT(float distance) const;\n\n    // Cubic interpolation between points\n    glm::vec3 interpolate(const glm::vec3&amp; p0, const glm::vec3&amp; p1, \n                         const glm::vec3&amp; p2, const glm::vec3&amp; p3, \n                         float t) const;\n};\n\n// Spline.cpp\n#include \"Spline.h\"\n#include &lt;algorithm&gt;\n\nvoid Spline::addPoint(const glm::vec3&amp; point) {\n    m_points.push_back(point);\n    updateDistances();\n}\n\nvoid Spline::updateDistances() {\n    m_distances.clear();\n    m_distances.push_back(0.0f);\n\n    for (size_t i = 1; i &lt; m_points.size(); ++i) {\n        float segmentLength = glm::length(m_points[i] - m_points[i-1]);\n        m_distances.push_back(m_distances.back() + segmentLength);\n    }\n}\n\nfloat Spline::distanceToT(float distance) const {\n    if (m_points.size() &lt; 2) return 0.0f;\n\n    // Find the segment containing this distance\n    auto it = std::lower_bound(m_distances.begin(), m_distances.end(), distance);\n    if (it == m_distances.begin()) return 0.0f;\n    if (it == m_distances.end()) return static_cast&lt;float&gt;(m_points.size() - 1);\n\n    // Get segment index and interpolation factor\n    size_t index = std::distance(m_distances.begin(), it) - 1;\n    float segmentStart = m_distances[index];\n    float segmentLength = m_distances[index + 1] - segmentStart;\n    float segmentT = (distance - segmentStart) / segmentLength;\n\n    return static_cast&lt;float&gt;(index) + segmentT;\n}\n\nglm::vec3 Spline::getPositionByDistance(float distance) const {\n    return getPositionByT(distanceToT(distance));\n}\n\nglm::vec3 Spline::getPositionByT(float t) const {\n    if (m_points.size() &lt; 2) {\n        return m_points.empty() ? glm::vec3(0.0f) : m_points[0];\n    }\n\n    // Clamp t to valid range\n    t = std::max(0.0f, std::min(t, static_cast&lt;float&gt;(m_points.size() - 1)));\n\n    // Find the segment containing t\n    size_t segment = static_cast&lt;size_t&gt;(t);\n    if (segment &gt;= m_points.size() - 1) {\n        return m_points.back();\n    }\n\n    // Get surrounding points for interpolation\n    glm::vec3 p0 = segment &gt; 0 ? m_points[segment - 1] : m_points[0];\n    glm::vec3 p1 = m_points[segment];\n    glm::vec3 p2 = m_points[segment + 1];\n    glm::vec3 p3 = segment &lt; m_points.size() - 2 ? m_points[segment + 2] : p2;\n\n    float localT = t - static_cast&lt;float&gt;(segment);\n    return interpolate(p0, p1, p2, p3, localT);\n}\n\nglm::vec3 Spline::interpolate(const glm::vec3&amp; p0, const glm::vec3&amp; p1,\n                             const glm::vec3&amp; p2, const glm::vec3&amp; p3,\n                             float t) const {\n    // Catmull-Rom spline interpolation\n    float t2 = t * t;\n    float t3 = t2 * t;\n\n    glm::vec3 a = p1;\n    glm::vec3 b = 0.5f * (p2 - p0);\n    glm::vec3 c = 0.5f * (2.0f * p0 - 5.0f * p1 + 4.0f * p2 - p3);\n    glm::vec3 d = 0.5f * (-p0 + 3.0f * p1 - 3.0f * p2 + p3);\n\n    return a + b * t + c * t2 + d * t3;\n}\n\nfloat Spline::getTotalLength() const {\n    return m_distances.empty() ? 0.0f : m_distances.back();\n}\n\nsize_t Spline::getPointCount() const {\n    return m_points.size();\n}\n\nvoid Spline::clear() {\n    m_points.clear();\n    m_distances.clear();\n}\n</code></pre>"},{"location":"blog/2025/01/25/coding-at-the-speed-of-light/#implementing-post-processing-effects","title":"Implementing post processing effects","text":"<p>As a creative coder, you always want to be able to add cool effects to your rendering. I wanted to add a post-processing pipeline to the rendering. I had some ideas on how to implement this, but I wanted to get some input from Claude on how to best implement this. As input, I also gave Claude the current rendering method in Ivf++ that I wanted to extend with post-processing effects.</p> <p>Question</p> <p>I have the following rendering method:</p> <p><pre><code>void GLFWSceneWindow::doDraw()\n{\n    GLFWWindow::doDraw();\n    glClearColor(0.07f, 0.13f, 0.17f, 1.0f);\n    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n    m_scene-&gt;draw();\n}\n</code></pre> that renders directly to a window. I would like to add post fx shaders after rendering. Suggest how this can be implemented.</p> <p>I got a very detailed answer with the basic approach and code examples. </p> <p>Answer</p> <p>To implement post-processing effects, you'll need to:</p> <ol> <li>Render your scene to a framebuffer object (FBO) instead of directly to the screen</li> <li>Create a full-screen quad to render the final result</li> <li>Apply post-processing shaders using the scene texture as input</li> </ol> <p>[ Example code ]</p> <p>Claude also provided hints and code on implementing a ping-pong buffer for the post-processing effects. This would have been a real pain to implement by reading documentation and tutorials. </p> <p>I also asked it to provide me with implementations for some common post-processing effects, such as blur, bloom, and tone mapping. Below is the code for the blur effect that it provided.</p> <pre><code>#version 330 core\nout vec4 FragColor;\nin vec2 TexCoords;\nuniform sampler2D screenTexture;\nuniform float blurSize = 0.003; // Adjust for blur strength\n\nvoid main()\n{\n    vec2 texelSize = blurSize / textureSize(screenTexture, 0);\n    vec3 result = vec3(0.0);\n\n    // 3x3 kernel\n    for(int x = -1; x &lt;= 1; x++) {\n        for(int y = -1; y &lt;= 1; y++) {\n            vec2 offset = vec2(float(x), float(y)) * texelSize;\n            result += texture(screenTexture, TexCoords + offset).rgb;\n        }\n    }\n\n    result = result / 9.0;\n    FragColor = vec4(result, 1.0);\n}\n</code></pre> <p>The following screenshot shows an example application with some post effects applied.</p> <p></p>"},{"location":"blog/2025/01/25/coding-at-the-speed-of-light/#conclusion","title":"Conclusion","text":"<p>By using AI tools like ChatGPT and Github Copilot, I have been able to implement features in Ivf++ 2.0 at a pace that I would not have been able to otherwise. Instead of getting stuck on implementation details, you can work on the same level as an architect or a designer. Provide a high-level design description and then let the AI assistant implement the finer details. This has given me the ability to focus on the design of the API and the overall architecture.</p> <p>Quote</p> <p>Coding at the speed of light is not about typing faster. It's about leveraging AI tools to focus on the high-level design and architecture of your software, while the AI takes care of the implementation details. This allows you to work at a pace that would not have been possible otherwise.</p> <p>AI tools will revolutionize software development by enhancing developer creativity and productivity. However, their effective use still requires deep domain knowledge and technical expertise. Rather than replacing fundamental skills, AI serves as a powerful accelerator for experienced developers who understand both the problem space and the capabilities of their tools. The future of software development lies in this synergy between human expertise and AI assistance.</p>"},{"location":"blog/2024/07/19/thoughts-the-forcepad-application/","title":"Thoughts the ForcePAD application","text":"<p>ForcePAD was developed as part of my PhD thesis almost 20 years ago. The application was developed in C++ using OpenGL for graphics and FLTK for the user interface. At the time, it was state-of-the-art and compiled on all platforms, such as Linux, Windows, and SGI Irix 6.5(!). The application was used extensively in teaching and is still used today.</p> <p>The application has been fixed and updated over the years an is available in the Windows app store. However, it is getting harder and harder to compile on both Windows and MacOS due to OpenGL being deprecated on the Mac and being complicated to maintain on Windows. The application would be ideal for running on an iPad, but as it uses OpenGL 1.1, which is not available on the iPad, this is also a dead end. </p> <p>Reflecting on my recent experience with the partial modernization of the older application, ObjectiveFrame, I see a promising path for the ForcePAD application. While the 3D graphics remain a challenge due to the reliance on a large scene graph library using OpenGL 1.1, the potential for modernization is clear. </p> <p>As the ForcePAD application uses a much simpler 2D graphics model, I have decided it is time for a blank sheet instead of a partial rewrite. This also gives me an opportunity to test some new ideas for the application:</p> <ul> <li>Layers for conceptual sketching, </li> <li>Blending</li> <li>Background speculative computations</li> <li>Immediate-mode user interfaces using ImGUI </li> <li>Upgraded multithreaded computational code using the Eigen C++ library. </li> </ul> <p>I will be publishing my development journey regularly on an upcoming blog. I will leave you with a short video with the user interface in its current state.</p>"},{"location":"blog/2024/08/21/the-conceptual-model-of-forcepad-3/","title":"The conceptual model of ForcePAD 3","text":"<p>When redesigning an application, there is an opportunity to rethink the existing design and make improvements. In the case of ForcePAD 2, it was a simple pixel-based drawing application with a single layer for drawing. Users could add loads and boundary conditions on top of the drawing, and results were displayed on top of the image layer.</p> <p>With ForcePAD 3, I aim to introduce a layered model that allows users to add multiple drawing layers on top of each other, similar to how a designer or architect uses sketch paper. These layers can be hidden and made transparent, providing more flexibility when sketching. In the following sections, I will provide a detailed description of the model and the main classes that implement it.</p>"},{"location":"blog/2024/08/21/the-conceptual-model-of-forcepad-3/#a-multi-layered-approach","title":"A multi layered approach","text":"<p>In ForcePAD 3, I aim to optimize drawing routines by leveraging the GPU more than in ForcePAD 2. To implement the layered model, I have chosen to utilize the render-to-texture capabilities in RayLib. This approach allows for easy implementation of multiple layers. The following code demonstrates the basic idea:</p> <pre><code>RenderTexture2D texture;\n\n...\n\nvoid draw()\n{\n    ...\n\n    BeginTextureMode(texture);\n\n    DrawEllipse(...)\n\n    EndTextureMode()\n\n    DrawTexture(texture, x, y, tint);\n\n    ...\n}\n</code></pre> <p>By rendering to a texture, each layer can be managed independently, providing flexibility and improved performance.</p> <pre><code>RenderTexture2D texture;\n\n...\n\nvoid draw()\n{\n    ...\n\n    BeginTextureMode(texture);\n\n    DrawEllipse(...)\n\n    EndTextureMode()\n\n    DrawTexture(texture, x, y, tint);\n\n    ...\n}\n</code></pre> <p>To abstract this model into something that can implement the layered model in ForcePAD 3, I decided to create a main Drawing class that contains multiple Layer instances, each managing a render texture for a specific layer. The handling of render textures is implemented in the RaylibRenderTexture class.</p> <p>The Drawing class has two methods, beginDraw() and endDraw(), which call the corresponding methods of the Layer class. This simplifies the main drawing routine in the application class:</p> <pre><code>void ForcePadWindow::onDraw()\n{\n    ClearBackground(WHITE);\n\n    m_drawing-&gt;beginDraw();\n\n    // Pixel drawing operations to current layer.\n\n    m_drawing-&gt;endDraw();\n\n    // Draw all textures\n\n    m_drawing-&gt;draw();\n}\n</code></pre> <p>This abstraction allows for easy management of multiple layers and improves the overall organization of the code.</p> <pre><code>void ForcePadWindow::onDraw()\n{\n    ClearBackground(WHITE);\n\n    // Initiate render to current layer and texture\n\n    m_drawing-&gt;beginDraw();\n\n    // Pixel drawing operations to current layer.\n\n    m_drawing-&gt;endDraw();\n\n    m_drawing-&gt;updateMouse(mouseX(), mouseY());\n\n    // Draw all textures\n\n    m_drawing-&gt;draw();\n}\n</code></pre> <p>The Drawing draw() method is also relatively small.</p> <pre><code>void Drawing::draw()\n{\n    ...\n\n    for (auto &amp;layer : m_layers)\n    {\n        if (layer-&gt;visible())\n        {\n            layer-&gt;draw();\n        }\n    }\n\n    ...\n}\n</code></pre> <p>I was initially concerned about the performance impact of using screen-sized render textures. However, after testing it on various hardware, I was pleasantly surprised to find that it performs remarkably well. The application consistently maintains a smooth frame rate of 60 frames per second, even when multiple layers are used.</p>"},{"location":"blog/2024/08/21/the-conceptual-model-of-forcepad-3/#adding-vector-drawing","title":"Adding vector drawing","text":"<p>To improve precision when sketching, I decided to implement vector-based drawing in ForcePAD. RayLib provides many built-in functions for drawing basic shapes, making it a suitable choice as a base for drawing the shapes. To achieve this, I created an abstract class Shape to store generic attributes such as fill color, line color, and border thickness for vector shapes. Additionally, I implemented specific Shape classes like Ellipse, Rectangle, and Line to implement concreate drawing of the shapes.</p> <p>To avoid implementing a separate vector drawing mechanism, each Layer class maintains a set of shapes that are drawn on top of the render texture. As a result, the draw() method of the layer becomes responsible for rendering these shapes.</p> <pre><code>void graphics::Layer::draw()\n{\n    m_renderTexture-&gt;draw(0, 0, m_tint);\n\n    for (auto &amp;shape : m_shapes)\n    {\n        shape-&gt;draw();\n    }\n}\n</code></pre>"},{"location":"blog/2024/08/21/the-conceptual-model-of-forcepad-3/#a-first-verion-of-the-forcepad-model","title":"A first verion of the ForcePAD model","text":"<p>The Drawing class will serve as the foundation for implementing the ForcePAD model. It will be a layered model consisting of the following layers:</p> <ol> <li>Drawing layers: Manages pixel and vector shapes.</li> <li>Result layer: Contains the results from the simulated model.</li> <li>Force and constraints layer: Draws and maintains forces and constraints for the model.</li> </ol> <p>The class will handle interactions with shapes, pixels, forces, and constraints. It will also implement features such as hovering and selection. Additionally, the Drawing class will serve as the main data model for ForcePAD, allowing for serialization of the model to disk. The plan is to use JSON as the primary file format for ForcePAD, enabling easy reading and writing of models by other applications. The implementation of serialization will be done in a separate class.</p> <p>I am satisfied with the current model and it has been successfully implemented in the latest version of ForcePAD 3 on GitHub. No significant modifications have been necessary for the underlying model as of yet.</p>"},{"location":"blog/2024/09/09/calfem-for-python-3610-released/","title":"CALFEM for Python 3.6.10 released","text":""},{"location":"blog/2024/09/09/calfem-for-python-3610-released/#distributing-and-packaging-calfem-for-python","title":"Distributing and packaging CALFEM for Python","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/opengl/","title":"OpenGL","text":""},{"location":"blog/category/c/","title":"C++","text":""},{"location":"blog/category/ivf/","title":"Ivf++","text":""},{"location":"blog/category/graphics/","title":"Graphics","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/claude/","title":"Claude","text":""},{"location":"blog/category/calfem/","title":"CALFEM","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/forcepad/","title":"ForcePAD","text":""},{"location":"blog/category/concept/","title":"Concept","text":""},{"location":"blog/category/design/","title":"Design","text":""},{"location":"blog/category/backend/","title":"Backend","text":""},{"location":"blog/category/raylib/","title":"Raylib","text":""},{"location":"blog/category/cmake/","title":"CMake","text":""},{"location":"blog/category/vcpkg/","title":"vcpkg","text":""},{"location":"blog/category/multiplatform/","title":"Multiplatform","text":""}]}